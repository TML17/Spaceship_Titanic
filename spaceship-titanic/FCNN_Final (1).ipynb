{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7499cea-f139-4bc9-bacb-a09bb0b265fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5d96bb-9773-43c1-aa69-1c5d722a11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import and split the training and testing dataset\n",
    "test_data = pd.read_csv(r'C:\\Users\\liuch\\Desktop\\395\\Final_Project\\Spaceship_Titanic\\spaceship-titanic\\test_data_featured.csv')\n",
    "\n",
    "# Change this comment if you want to pick from mode imputation or knn imputation\n",
    "train_data_mode = pd.read_csv(r'C:\\Users\\liuch\\Desktop\\395\\Final_Project\\Spaceship_Titanic\\spaceship-titanic\\train_data_mode.csv')\n",
    "## train_data_mode = pd.read_csv(r'C:\\Users\\liuch\\Desktop\\395\\Final_Project\\Spaceship_Titanic\\spaceship-titanic\\train_data_knn.csv')\n",
    "\n",
    "train_data_mode.drop(['PassengerId', 'Name', 'Cabin'], axis=1, inplace=True)\n",
    "test_data.drop(['PassengerId', 'Name', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "train_mode_y = train_data_mode['Transported']\n",
    "train_mode_X = train_data_mode.drop('Transported',axis=1)\n",
    "test_data_y = test_data['Transported']\n",
    "test_data_X = test_data.drop('Transported',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe0ec03-e16d-452a-bfb4-c2e351f53627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CryoSleep                    float64\n",
      "Age                          float64\n",
      "VIP                          float64\n",
      "RoomService                  float64\n",
      "FoodCourt                    float64\n",
      "ShoppingMall                 float64\n",
      "Spa                          float64\n",
      "VRDeck                       float64\n",
      "Num                          float64\n",
      "Deck_A                       float64\n",
      "Deck_B                       float64\n",
      "Deck_C                       float64\n",
      "Deck_D                       float64\n",
      "Deck_E                       float64\n",
      "Deck_F                       float64\n",
      "Deck_G                       float64\n",
      "Deck_T                       float64\n",
      "Side_P                       float64\n",
      "Side_S                       float64\n",
      "Destination_55 Cancri e      float64\n",
      "Destination_PSO J318.5-22    float64\n",
      "Destination_TRAPPIST-1e      float64\n",
      "HomePlanet_Earth             float64\n",
      "HomePlanet_Europa            float64\n",
      "HomePlanet_Mars              float64\n",
      "Cabin_encoded                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert Boolean Columns to Floats\n",
    "for col in train_mode_X.columns:\n",
    "    if train_mode_X[col].dtype == bool:\n",
    "        train_mode_X[col] = train_mode_X[col].astype(float)\n",
    "\n",
    "print(train_mode_X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a29553a-dadb-41cc-9596-0e520e1b4dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CryoSleep                    float64\n",
      "Age                          float64\n",
      "VIP                          float64\n",
      "RoomService                  float64\n",
      "FoodCourt                    float64\n",
      "ShoppingMall                 float64\n",
      "Spa                          float64\n",
      "VRDeck                       float64\n",
      "Num                          float64\n",
      "Deck_A                       float64\n",
      "Deck_B                       float64\n",
      "Deck_C                       float64\n",
      "Deck_D                       float64\n",
      "Deck_E                       float64\n",
      "Deck_F                       float64\n",
      "Deck_G                       float64\n",
      "Deck_T                       float64\n",
      "Side_P                       float64\n",
      "Side_S                       float64\n",
      "Destination_55 Cancri e      float64\n",
      "Destination_PSO J318.5-22    float64\n",
      "Destination_TRAPPIST-1e      float64\n",
      "HomePlanet_Earth             float64\n",
      "HomePlanet_Europa            float64\n",
      "HomePlanet_Mars              float64\n",
      "Cabin_encoded                  int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert Boolean Columns to Floats\n",
    "for col in test_data_X.columns:\n",
    "    if test_data_X[col].dtype == bool or test_data_X[col].dtype == object:\n",
    "        test_data_X[col] = test_data_X[col].astype(float)\n",
    "\n",
    "print(test_data_X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba3535d-8962-4c75-8e69-5699ecb031cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframes to numpy arrays and then to PyTorch tensors\n",
    "train_mode_X_tensor = torch.tensor(train_mode_X.values.astype(np.float32))\n",
    "train_mode_y_tensor = torch.tensor(train_mode_y.values.astype(np.float32))\n",
    "test_data_X_tensor = torch.tensor(test_data_X.values.astype(np.float32))\n",
    "test_data_y_tensor = torch.tensor(test_data_y.values.astype(np.float32))\n",
    "\n",
    "# Create Tensor datasets\n",
    "train_dataset = TensorDataset(train_mode_X_tensor, train_mode_y_tensor)\n",
    "test_dataset = TensorDataset(test_data_X_tensor, test_data_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cc2aeb-d034-4028-9260-5a846479ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "# Create the model instance with the correct input size\n",
    "input_size = train_mode_X_tensor.shape[1]\n",
    "model = BinaryClassifier(input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0e244e-fd55-4c8d-b2f8-66603e8606a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 Loss: 0.8033303022384644\n",
      "Epoch 2/500 Loss: 0.53824383020401\n",
      "Epoch 3/500 Loss: 0.6730542182922363\n",
      "Epoch 4/500 Loss: 0.5004540085792542\n",
      "Epoch 5/500 Loss: 0.4031817615032196\n",
      "Epoch 6/500 Loss: 0.6808675527572632\n",
      "Epoch 7/500 Loss: 0.5225037932395935\n",
      "Epoch 8/500 Loss: 0.4470587372779846\n",
      "Epoch 9/500 Loss: 0.4689681828022003\n",
      "Epoch 10/500 Loss: 0.4952312707901001\n",
      "Epoch 11/500 Loss: 0.6034083366394043\n",
      "Epoch 12/500 Loss: 0.37474751472473145\n",
      "Epoch 13/500 Loss: 0.5920603275299072\n",
      "Epoch 14/500 Loss: 0.4045535922050476\n",
      "Epoch 15/500 Loss: 0.4223123788833618\n",
      "Epoch 16/500 Loss: 0.4185619354248047\n",
      "Epoch 17/500 Loss: 0.6873879432678223\n",
      "Epoch 18/500 Loss: 0.5560809373855591\n",
      "Epoch 19/500 Loss: 0.7511594891548157\n",
      "Epoch 20/500 Loss: 0.4917945861816406\n",
      "Epoch 21/500 Loss: 0.7659285664558411\n",
      "Epoch 22/500 Loss: 0.3558990955352783\n",
      "Epoch 23/500 Loss: 0.5413002967834473\n",
      "Epoch 24/500 Loss: 0.43038687109947205\n",
      "Epoch 25/500 Loss: 0.46577996015548706\n",
      "Epoch 26/500 Loss: 0.5433860421180725\n",
      "Epoch 27/500 Loss: 0.6950299739837646\n",
      "Epoch 28/500 Loss: 0.3420124650001526\n",
      "Epoch 29/500 Loss: 0.3801761865615845\n",
      "Epoch 30/500 Loss: 0.36271733045578003\n",
      "Epoch 31/500 Loss: 0.5428427457809448\n",
      "Epoch 32/500 Loss: 0.29835301637649536\n",
      "Epoch 33/500 Loss: 0.4243081510066986\n",
      "Epoch 34/500 Loss: 0.46278607845306396\n",
      "Epoch 35/500 Loss: 0.5096078515052795\n",
      "Epoch 36/500 Loss: 0.40223994851112366\n",
      "Epoch 37/500 Loss: 0.48917222023010254\n",
      "Epoch 38/500 Loss: 0.46607154607772827\n",
      "Epoch 39/500 Loss: 0.575707197189331\n",
      "Epoch 40/500 Loss: 0.4682777225971222\n",
      "Epoch 41/500 Loss: 0.46548783779144287\n",
      "Epoch 42/500 Loss: 0.36183980107307434\n",
      "Epoch 43/500 Loss: 0.5950314998626709\n",
      "Epoch 44/500 Loss: 0.26053354144096375\n",
      "Epoch 45/500 Loss: 0.407599538564682\n",
      "Epoch 46/500 Loss: 0.43478426337242126\n",
      "Epoch 47/500 Loss: 0.4693830609321594\n",
      "Epoch 48/500 Loss: 0.5081238746643066\n",
      "Epoch 49/500 Loss: 0.6204585433006287\n",
      "Epoch 50/500 Loss: 0.5633691549301147\n",
      "Epoch 51/500 Loss: 0.4332619309425354\n",
      "Epoch 52/500 Loss: 0.41960322856903076\n",
      "Epoch 53/500 Loss: 0.6616941094398499\n",
      "Epoch 54/500 Loss: 0.3040887415409088\n",
      "Epoch 55/500 Loss: 0.4448436498641968\n",
      "Epoch 56/500 Loss: 0.37551143765449524\n",
      "Epoch 57/500 Loss: 0.3556877076625824\n",
      "Epoch 58/500 Loss: 0.535683274269104\n",
      "Epoch 59/500 Loss: 0.4376544952392578\n",
      "Epoch 60/500 Loss: 0.5884476900100708\n",
      "Epoch 61/500 Loss: 0.37181904911994934\n",
      "Epoch 62/500 Loss: 0.4719802737236023\n",
      "Epoch 63/500 Loss: 0.3527645766735077\n",
      "Epoch 64/500 Loss: 0.34085461497306824\n",
      "Epoch 65/500 Loss: 0.42047059535980225\n",
      "Epoch 66/500 Loss: 0.500539243221283\n",
      "Epoch 67/500 Loss: 0.3407875597476959\n",
      "Epoch 68/500 Loss: 0.3566930592060089\n",
      "Epoch 69/500 Loss: 0.3442384600639343\n",
      "Epoch 70/500 Loss: 0.3476763367652893\n",
      "Epoch 71/500 Loss: 0.37020760774612427\n",
      "Epoch 72/500 Loss: 0.4490683376789093\n",
      "Epoch 73/500 Loss: 0.445124089717865\n",
      "Epoch 74/500 Loss: 0.40627148747444153\n",
      "Epoch 75/500 Loss: 0.5396737456321716\n",
      "Epoch 76/500 Loss: 0.5133081078529358\n",
      "Epoch 77/500 Loss: 0.7734638452529907\n",
      "Epoch 78/500 Loss: 0.4672100841999054\n",
      "Epoch 79/500 Loss: 0.27364450693130493\n",
      "Epoch 80/500 Loss: 0.5788623094558716\n",
      "Epoch 81/500 Loss: 0.396138459444046\n",
      "Epoch 82/500 Loss: 0.6281780004501343\n",
      "Epoch 83/500 Loss: 0.5501636266708374\n",
      "Epoch 84/500 Loss: 0.49428460001945496\n",
      "Epoch 85/500 Loss: 0.4244484305381775\n",
      "Epoch 86/500 Loss: 0.3849721848964691\n",
      "Epoch 87/500 Loss: 0.3914749026298523\n",
      "Epoch 88/500 Loss: 0.6124608516693115\n",
      "Epoch 89/500 Loss: 0.44001996517181396\n",
      "Epoch 90/500 Loss: 0.4496316909790039\n",
      "Epoch 91/500 Loss: 0.3649500012397766\n",
      "Epoch 92/500 Loss: 0.4333151876926422\n",
      "Epoch 93/500 Loss: 0.5202208757400513\n",
      "Epoch 94/500 Loss: 0.2674194574356079\n",
      "Epoch 95/500 Loss: 0.3147212862968445\n",
      "Epoch 96/500 Loss: 0.49097561836242676\n",
      "Epoch 97/500 Loss: 0.3490796685218811\n",
      "Epoch 98/500 Loss: 0.4715767502784729\n",
      "Epoch 99/500 Loss: 0.32102829217910767\n",
      "Epoch 100/500 Loss: 0.3841179609298706\n",
      "Epoch 101/500 Loss: 0.559535801410675\n",
      "Epoch 102/500 Loss: 0.45144036412239075\n",
      "Epoch 103/500 Loss: 0.5138199925422668\n",
      "Epoch 104/500 Loss: 0.5702911615371704\n",
      "Epoch 105/500 Loss: 0.4445722699165344\n",
      "Epoch 106/500 Loss: 0.30570825934410095\n",
      "Epoch 107/500 Loss: 0.23643675446510315\n",
      "Epoch 108/500 Loss: 0.23256224393844604\n",
      "Epoch 109/500 Loss: 0.41867807507514954\n",
      "Epoch 110/500 Loss: 0.5844102501869202\n",
      "Epoch 111/500 Loss: 0.32729554176330566\n",
      "Epoch 112/500 Loss: 0.6460424065589905\n",
      "Epoch 113/500 Loss: 0.5383039712905884\n",
      "Epoch 114/500 Loss: 0.3557054102420807\n",
      "Epoch 115/500 Loss: 0.6387166380882263\n",
      "Epoch 116/500 Loss: 0.47991862893104553\n",
      "Epoch 117/500 Loss: 0.47567975521087646\n",
      "Epoch 118/500 Loss: 0.500507116317749\n",
      "Epoch 119/500 Loss: 0.36347779631614685\n",
      "Epoch 120/500 Loss: 0.5005744099617004\n",
      "Epoch 121/500 Loss: 0.3329585790634155\n",
      "Epoch 122/500 Loss: 0.360449880361557\n",
      "Epoch 123/500 Loss: 0.2808493971824646\n",
      "Epoch 124/500 Loss: 0.3505135178565979\n",
      "Epoch 125/500 Loss: 0.3083846867084503\n",
      "Epoch 126/500 Loss: 0.21497154235839844\n",
      "Epoch 127/500 Loss: 0.5852928161621094\n",
      "Epoch 128/500 Loss: 0.21786563098430634\n",
      "Epoch 129/500 Loss: 0.5603199005126953\n",
      "Epoch 130/500 Loss: 0.30031606554985046\n",
      "Epoch 131/500 Loss: 0.2661088705062866\n",
      "Epoch 132/500 Loss: 0.4935508668422699\n",
      "Epoch 133/500 Loss: 0.7317820191383362\n",
      "Epoch 134/500 Loss: 0.23713044822216034\n",
      "Epoch 135/500 Loss: 0.4157972037792206\n",
      "Epoch 136/500 Loss: 0.3394627273082733\n",
      "Epoch 137/500 Loss: 0.360144704580307\n",
      "Epoch 138/500 Loss: 0.40143322944641113\n",
      "Epoch 139/500 Loss: 0.509189248085022\n",
      "Epoch 140/500 Loss: 0.4722622036933899\n",
      "Epoch 141/500 Loss: 0.5129059553146362\n",
      "Epoch 142/500 Loss: 0.3250046372413635\n",
      "Epoch 143/500 Loss: 0.4200983941555023\n",
      "Epoch 144/500 Loss: 0.4157036542892456\n",
      "Epoch 145/500 Loss: 0.29323118925094604\n",
      "Epoch 146/500 Loss: 0.39161917567253113\n",
      "Epoch 147/500 Loss: 0.6804934740066528\n",
      "Epoch 148/500 Loss: 0.4345049262046814\n",
      "Epoch 149/500 Loss: 0.3576239049434662\n",
      "Epoch 150/500 Loss: 0.2847921848297119\n",
      "Epoch 151/500 Loss: 0.5346941947937012\n",
      "Epoch 152/500 Loss: 0.4836196005344391\n",
      "Epoch 153/500 Loss: 0.3777589201927185\n",
      "Epoch 154/500 Loss: 0.47691500186920166\n",
      "Epoch 155/500 Loss: 0.3864377439022064\n",
      "Epoch 156/500 Loss: 0.6311866641044617\n",
      "Epoch 157/500 Loss: 0.3235238492488861\n",
      "Epoch 158/500 Loss: 0.4044518768787384\n",
      "Epoch 159/500 Loss: 0.4620278775691986\n",
      "Epoch 160/500 Loss: 0.45791301131248474\n",
      "Epoch 161/500 Loss: 0.443089097738266\n",
      "Epoch 162/500 Loss: 0.49533745646476746\n",
      "Epoch 163/500 Loss: 0.28045740723609924\n",
      "Epoch 164/500 Loss: 0.3391321301460266\n",
      "Epoch 165/500 Loss: 0.3716067969799042\n",
      "Epoch 166/500 Loss: 0.45268094539642334\n",
      "Epoch 167/500 Loss: 0.4877631366252899\n",
      "Epoch 168/500 Loss: 0.40225276350975037\n",
      "Epoch 169/500 Loss: 0.4808844327926636\n",
      "Epoch 170/500 Loss: 0.3985503613948822\n",
      "Epoch 171/500 Loss: 0.2251959592103958\n",
      "Epoch 172/500 Loss: 0.23167738318443298\n",
      "Epoch 173/500 Loss: 0.4645410478115082\n",
      "Epoch 174/500 Loss: 0.2482045739889145\n",
      "Epoch 175/500 Loss: 0.569266140460968\n",
      "Epoch 176/500 Loss: 0.33667632937431335\n",
      "Epoch 177/500 Loss: 0.5292573571205139\n",
      "Epoch 178/500 Loss: 0.4214860498905182\n",
      "Epoch 179/500 Loss: 0.34458819031715393\n",
      "Epoch 180/500 Loss: 0.3683300018310547\n",
      "Epoch 181/500 Loss: 0.26724347472190857\n",
      "Epoch 182/500 Loss: 0.5460140705108643\n",
      "Epoch 183/500 Loss: 0.43055230379104614\n",
      "Epoch 184/500 Loss: 0.45903393626213074\n",
      "Epoch 185/500 Loss: 0.2989939749240875\n",
      "Epoch 186/500 Loss: 0.4080788791179657\n",
      "Epoch 187/500 Loss: 0.4345552325248718\n",
      "Epoch 188/500 Loss: 0.3080414831638336\n",
      "Epoch 189/500 Loss: 0.29328569769859314\n",
      "Epoch 190/500 Loss: 0.5046941041946411\n",
      "Epoch 191/500 Loss: 0.23570698499679565\n",
      "Epoch 192/500 Loss: 0.37736475467681885\n",
      "Epoch 193/500 Loss: 0.3743603825569153\n",
      "Epoch 194/500 Loss: 0.32245758175849915\n",
      "Epoch 195/500 Loss: 0.3597407341003418\n",
      "Epoch 196/500 Loss: 0.24622520804405212\n",
      "Epoch 197/500 Loss: 0.43422436714172363\n",
      "Epoch 198/500 Loss: 0.2727229595184326\n",
      "Epoch 199/500 Loss: 0.23766468465328217\n",
      "Epoch 200/500 Loss: 0.5786189436912537\n",
      "Epoch 201/500 Loss: 0.38865000009536743\n",
      "Epoch 202/500 Loss: 0.22425642609596252\n",
      "Epoch 203/500 Loss: 0.40075749158859253\n",
      "Epoch 204/500 Loss: 0.34479668736457825\n",
      "Epoch 205/500 Loss: 0.21561218798160553\n",
      "Epoch 206/500 Loss: 0.35468795895576477\n",
      "Epoch 207/500 Loss: 0.3442501425743103\n",
      "Epoch 208/500 Loss: 0.7093945145606995\n",
      "Epoch 209/500 Loss: 0.35196027159690857\n",
      "Epoch 210/500 Loss: 0.37397605180740356\n",
      "Epoch 211/500 Loss: 0.34430667757987976\n",
      "Epoch 212/500 Loss: 0.44092392921447754\n",
      "Epoch 213/500 Loss: 0.7462388873100281\n",
      "Epoch 214/500 Loss: 0.5533802509307861\n",
      "Epoch 215/500 Loss: 0.5627121925354004\n",
      "Epoch 216/500 Loss: 0.38608551025390625\n",
      "Epoch 217/500 Loss: 0.36077767610549927\n",
      "Epoch 218/500 Loss: 0.27135494351387024\n",
      "Epoch 219/500 Loss: 0.4096546173095703\n",
      "Epoch 220/500 Loss: 0.4231364130973816\n",
      "Epoch 221/500 Loss: 0.6058514714241028\n",
      "Epoch 222/500 Loss: 0.4714134931564331\n",
      "Epoch 223/500 Loss: 0.36184197664260864\n",
      "Epoch 224/500 Loss: 0.39812952280044556\n",
      "Epoch 225/500 Loss: 0.5550649166107178\n",
      "Epoch 226/500 Loss: 0.47303760051727295\n",
      "Epoch 227/500 Loss: 0.27490097284317017\n",
      "Epoch 228/500 Loss: 0.3922802209854126\n",
      "Epoch 229/500 Loss: 0.2225894033908844\n",
      "Epoch 230/500 Loss: 0.4753419756889343\n",
      "Epoch 231/500 Loss: 0.4267957806587219\n",
      "Epoch 232/500 Loss: 0.5192605257034302\n",
      "Epoch 233/500 Loss: 0.5754172205924988\n",
      "Epoch 234/500 Loss: 0.27346739172935486\n",
      "Epoch 235/500 Loss: 0.4223797619342804\n",
      "Epoch 236/500 Loss: 0.3296301066875458\n",
      "Epoch 237/500 Loss: 0.29934343695640564\n",
      "Epoch 238/500 Loss: 0.34913745522499084\n",
      "Epoch 239/500 Loss: 0.5818386077880859\n",
      "Epoch 240/500 Loss: 0.39241644740104675\n",
      "Epoch 241/500 Loss: 0.41203299164772034\n",
      "Epoch 242/500 Loss: 0.42077532410621643\n",
      "Epoch 243/500 Loss: 0.5059078931808472\n",
      "Epoch 244/500 Loss: 0.5602416396141052\n",
      "Epoch 245/500 Loss: 0.5434337258338928\n",
      "Epoch 246/500 Loss: 0.3691711127758026\n",
      "Epoch 247/500 Loss: 0.6554905772209167\n",
      "Epoch 248/500 Loss: 0.3615533411502838\n",
      "Epoch 249/500 Loss: 0.4192766547203064\n",
      "Epoch 250/500 Loss: 0.3498632311820984\n",
      "Epoch 251/500 Loss: 0.3874306380748749\n",
      "Epoch 252/500 Loss: 0.48987334966659546\n",
      "Epoch 253/500 Loss: 0.44051724672317505\n",
      "Epoch 254/500 Loss: 0.3691725432872772\n",
      "Epoch 255/500 Loss: 0.5924003720283508\n",
      "Epoch 256/500 Loss: 0.38915154337882996\n",
      "Epoch 257/500 Loss: 0.45295771956443787\n",
      "Epoch 258/500 Loss: 0.23699694871902466\n",
      "Epoch 259/500 Loss: 0.35773277282714844\n",
      "Epoch 260/500 Loss: 0.6450652480125427\n",
      "Epoch 261/500 Loss: 0.5159613490104675\n",
      "Epoch 262/500 Loss: 0.4129471480846405\n",
      "Epoch 263/500 Loss: 0.48860442638397217\n",
      "Epoch 264/500 Loss: 0.37733057141304016\n",
      "Epoch 265/500 Loss: 0.4194321930408478\n",
      "Epoch 266/500 Loss: 0.29080426692962646\n",
      "Epoch 267/500 Loss: 0.25487372279167175\n",
      "Epoch 268/500 Loss: 0.4804703891277313\n",
      "Epoch 269/500 Loss: 0.4782761335372925\n",
      "Epoch 270/500 Loss: 0.41768521070480347\n",
      "Epoch 271/500 Loss: 0.6835372447967529\n",
      "Epoch 272/500 Loss: 0.5174055695533752\n",
      "Epoch 273/500 Loss: 0.4681026041507721\n",
      "Epoch 274/500 Loss: 0.46994084119796753\n",
      "Epoch 275/500 Loss: 0.20517022907733917\n",
      "Epoch 276/500 Loss: 0.5945781469345093\n",
      "Epoch 277/500 Loss: 0.30494755506515503\n",
      "Epoch 278/500 Loss: 0.7094116806983948\n",
      "Epoch 279/500 Loss: 0.39381200075149536\n",
      "Epoch 280/500 Loss: 0.2696533799171448\n",
      "Epoch 281/500 Loss: 0.43989092111587524\n",
      "Epoch 282/500 Loss: 0.2528102993965149\n",
      "Epoch 283/500 Loss: 0.34218984842300415\n",
      "Epoch 284/500 Loss: 0.608904242515564\n",
      "Epoch 285/500 Loss: 0.30432796478271484\n",
      "Epoch 286/500 Loss: 0.44382742047309875\n",
      "Epoch 287/500 Loss: 0.5210744142532349\n",
      "Epoch 288/500 Loss: 0.2782176434993744\n",
      "Epoch 289/500 Loss: 0.39776867628097534\n",
      "Epoch 290/500 Loss: 0.7309197783470154\n",
      "Epoch 291/500 Loss: 0.41844984889030457\n",
      "Epoch 292/500 Loss: 0.4103342890739441\n",
      "Epoch 293/500 Loss: 0.41465938091278076\n",
      "Epoch 294/500 Loss: 0.4344920217990875\n",
      "Epoch 295/500 Loss: 0.34416812658309937\n",
      "Epoch 296/500 Loss: 0.6155892014503479\n",
      "Epoch 297/500 Loss: 0.29172438383102417\n",
      "Epoch 298/500 Loss: 0.4140429198741913\n",
      "Epoch 299/500 Loss: 0.4345409870147705\n",
      "Epoch 300/500 Loss: 0.5465202927589417\n",
      "Epoch 301/500 Loss: 0.5069522261619568\n",
      "Epoch 302/500 Loss: 0.6157966256141663\n",
      "Epoch 303/500 Loss: 0.2938235104084015\n",
      "Epoch 304/500 Loss: 0.2041936218738556\n",
      "Epoch 305/500 Loss: 0.4657859802246094\n",
      "Epoch 306/500 Loss: 0.63968825340271\n",
      "Epoch 307/500 Loss: 0.5246897339820862\n",
      "Epoch 308/500 Loss: 0.3021749258041382\n",
      "Epoch 309/500 Loss: 0.28963950276374817\n",
      "Epoch 310/500 Loss: 0.24784308671951294\n",
      "Epoch 311/500 Loss: 0.3711162805557251\n",
      "Epoch 312/500 Loss: 0.4175291657447815\n",
      "Epoch 313/500 Loss: 0.3372165560722351\n",
      "Epoch 314/500 Loss: 0.8895714282989502\n",
      "Epoch 315/500 Loss: 0.2915985882282257\n",
      "Epoch 316/500 Loss: 0.5182486772537231\n",
      "Epoch 317/500 Loss: 0.476792573928833\n",
      "Epoch 318/500 Loss: 0.3654513359069824\n",
      "Epoch 319/500 Loss: 0.40088945627212524\n",
      "Epoch 320/500 Loss: 0.5137737393379211\n",
      "Epoch 321/500 Loss: 0.16504763066768646\n",
      "Epoch 322/500 Loss: 0.3419121503829956\n",
      "Epoch 323/500 Loss: 0.24004748463630676\n",
      "Epoch 324/500 Loss: 0.31629371643066406\n",
      "Epoch 325/500 Loss: 0.5255454778671265\n",
      "Epoch 326/500 Loss: 0.434386670589447\n",
      "Epoch 327/500 Loss: 0.3099924325942993\n",
      "Epoch 328/500 Loss: 0.38192713260650635\n",
      "Epoch 329/500 Loss: 0.3080742657184601\n",
      "Epoch 330/500 Loss: 0.5063318610191345\n",
      "Epoch 331/500 Loss: 0.3368251621723175\n",
      "Epoch 332/500 Loss: 0.5118861794471741\n",
      "Epoch 333/500 Loss: 0.47758111357688904\n",
      "Epoch 334/500 Loss: 0.3406526744365692\n",
      "Epoch 335/500 Loss: 0.16204538941383362\n",
      "Epoch 336/500 Loss: 0.5685060024261475\n",
      "Epoch 337/500 Loss: 0.35428279638290405\n",
      "Epoch 338/500 Loss: 0.5774958729743958\n",
      "Epoch 339/500 Loss: 0.367685467004776\n",
      "Epoch 340/500 Loss: 0.2824970483779907\n",
      "Epoch 341/500 Loss: 0.5429479479789734\n",
      "Epoch 342/500 Loss: 0.2898504137992859\n",
      "Epoch 343/500 Loss: 0.30191442370414734\n",
      "Epoch 344/500 Loss: 0.47066831588745117\n",
      "Epoch 345/500 Loss: 0.32328373193740845\n",
      "Epoch 346/500 Loss: 0.5404703617095947\n",
      "Epoch 347/500 Loss: 0.27518004179000854\n",
      "Epoch 348/500 Loss: 0.4373475909233093\n",
      "Epoch 349/500 Loss: 0.4086577296257019\n",
      "Epoch 350/500 Loss: 0.4367082715034485\n",
      "Epoch 351/500 Loss: 0.5845984220504761\n",
      "Epoch 352/500 Loss: 0.4088837504386902\n",
      "Epoch 353/500 Loss: 0.34506088495254517\n",
      "Epoch 354/500 Loss: 0.5992008447647095\n",
      "Epoch 355/500 Loss: 0.2683699429035187\n",
      "Epoch 356/500 Loss: 0.3466976284980774\n",
      "Epoch 357/500 Loss: 0.31338298320770264\n",
      "Epoch 358/500 Loss: 0.34589365124702454\n",
      "Epoch 359/500 Loss: 0.29524487257003784\n",
      "Epoch 360/500 Loss: 0.2837482690811157\n",
      "Epoch 361/500 Loss: 0.25997546315193176\n",
      "Epoch 362/500 Loss: 0.3545166552066803\n",
      "Epoch 363/500 Loss: 0.24802444875240326\n",
      "Epoch 364/500 Loss: 0.36998313665390015\n",
      "Epoch 365/500 Loss: 0.4291270077228546\n",
      "Epoch 366/500 Loss: 0.3263978362083435\n",
      "Epoch 367/500 Loss: 0.4637439548969269\n",
      "Epoch 368/500 Loss: 0.4658271372318268\n",
      "Epoch 369/500 Loss: 0.41470247507095337\n",
      "Epoch 370/500 Loss: 0.37109991908073425\n",
      "Epoch 371/500 Loss: 0.47498294711112976\n",
      "Epoch 372/500 Loss: 0.31511980295181274\n",
      "Epoch 373/500 Loss: 0.5866043567657471\n",
      "Epoch 374/500 Loss: 0.608931303024292\n",
      "Epoch 375/500 Loss: 0.5108361840248108\n",
      "Epoch 376/500 Loss: 0.4298076629638672\n",
      "Epoch 377/500 Loss: 0.2836463749408722\n",
      "Epoch 378/500 Loss: 0.3073907196521759\n",
      "Epoch 379/500 Loss: 0.5103419423103333\n",
      "Epoch 380/500 Loss: 0.40552976727485657\n",
      "Epoch 381/500 Loss: 0.30823853611946106\n",
      "Epoch 382/500 Loss: 0.32107970118522644\n",
      "Epoch 383/500 Loss: 0.4399920403957367\n",
      "Epoch 384/500 Loss: 0.48936599493026733\n",
      "Epoch 385/500 Loss: 0.2648485004901886\n",
      "Epoch 386/500 Loss: 0.32134175300598145\n",
      "Epoch 387/500 Loss: 0.2621488869190216\n",
      "Epoch 388/500 Loss: 0.5059589147567749\n",
      "Epoch 389/500 Loss: 0.5750470161437988\n",
      "Epoch 390/500 Loss: 0.41950753331184387\n",
      "Epoch 391/500 Loss: 0.3867669403553009\n",
      "Epoch 392/500 Loss: 0.3470478057861328\n",
      "Epoch 393/500 Loss: 0.46012774109840393\n",
      "Epoch 394/500 Loss: 0.39730304479599\n",
      "Epoch 395/500 Loss: 0.2679700553417206\n",
      "Epoch 396/500 Loss: 0.3934344947338104\n",
      "Epoch 397/500 Loss: 0.40410715341567993\n",
      "Epoch 398/500 Loss: 0.33489540219306946\n",
      "Epoch 399/500 Loss: 0.3355834484100342\n",
      "Epoch 400/500 Loss: 0.24033614993095398\n",
      "Epoch 401/500 Loss: 0.4824711084365845\n",
      "Epoch 402/500 Loss: 0.5289822816848755\n",
      "Epoch 403/500 Loss: 0.49341410398483276\n",
      "Epoch 404/500 Loss: 0.5669612288475037\n",
      "Epoch 405/500 Loss: 0.4308049976825714\n",
      "Epoch 406/500 Loss: 0.46172410249710083\n",
      "Epoch 407/500 Loss: 0.3527013063430786\n",
      "Epoch 408/500 Loss: 0.33016249537467957\n",
      "Epoch 409/500 Loss: 0.48813098669052124\n",
      "Epoch 410/500 Loss: 0.22453449666500092\n",
      "Epoch 411/500 Loss: 0.46419644355773926\n",
      "Epoch 412/500 Loss: 0.4636909067630768\n",
      "Epoch 413/500 Loss: 0.30053335428237915\n",
      "Epoch 414/500 Loss: 0.3893194794654846\n",
      "Epoch 415/500 Loss: 0.4302341043949127\n",
      "Epoch 416/500 Loss: 0.6048635244369507\n",
      "Epoch 417/500 Loss: 0.42711761593818665\n",
      "Epoch 418/500 Loss: 0.72837233543396\n",
      "Epoch 419/500 Loss: 0.21196302771568298\n",
      "Epoch 420/500 Loss: 0.39774927496910095\n",
      "Epoch 421/500 Loss: 0.28704485297203064\n",
      "Epoch 422/500 Loss: 0.463426411151886\n",
      "Epoch 423/500 Loss: 0.26558926701545715\n",
      "Epoch 424/500 Loss: 0.3966217041015625\n",
      "Epoch 425/500 Loss: 0.650332510471344\n",
      "Epoch 426/500 Loss: 0.39757126569747925\n",
      "Epoch 427/500 Loss: 0.30906158685684204\n",
      "Epoch 428/500 Loss: 0.3154081702232361\n",
      "Epoch 429/500 Loss: 0.3920355439186096\n",
      "Epoch 430/500 Loss: 0.468661904335022\n",
      "Epoch 431/500 Loss: 0.46802976727485657\n",
      "Epoch 432/500 Loss: 0.4876903295516968\n",
      "Epoch 433/500 Loss: 0.4230886399745941\n",
      "Epoch 434/500 Loss: 0.3836441934108734\n",
      "Epoch 435/500 Loss: 0.2402126044034958\n",
      "Epoch 436/500 Loss: 0.278846800327301\n",
      "Epoch 437/500 Loss: 0.5305013060569763\n",
      "Epoch 438/500 Loss: 0.2953473627567291\n",
      "Epoch 439/500 Loss: 0.6410554647445679\n",
      "Epoch 440/500 Loss: 0.4964264929294586\n",
      "Epoch 441/500 Loss: 0.40104612708091736\n",
      "Epoch 442/500 Loss: 0.36146315932273865\n",
      "Epoch 443/500 Loss: 0.481599360704422\n",
      "Epoch 444/500 Loss: 0.31343895196914673\n",
      "Epoch 445/500 Loss: 0.22549018263816833\n",
      "Epoch 446/500 Loss: 0.25568604469299316\n",
      "Epoch 447/500 Loss: 0.2732948362827301\n",
      "Epoch 448/500 Loss: 0.6154069304466248\n",
      "Epoch 449/500 Loss: 0.22650684416294098\n",
      "Epoch 450/500 Loss: 0.46500125527381897\n",
      "Epoch 451/500 Loss: 0.17037910223007202\n",
      "Epoch 452/500 Loss: 0.4094984829425812\n",
      "Epoch 453/500 Loss: 0.2992609441280365\n",
      "Epoch 454/500 Loss: 0.38504329323768616\n",
      "Epoch 455/500 Loss: 0.39784422516822815\n",
      "Epoch 456/500 Loss: 0.43097132444381714\n",
      "Epoch 457/500 Loss: 0.48095130920410156\n",
      "Epoch 458/500 Loss: 0.39593246579170227\n",
      "Epoch 459/500 Loss: 0.4793435037136078\n",
      "Epoch 460/500 Loss: 0.4444165527820587\n",
      "Epoch 461/500 Loss: 0.45094063878059387\n",
      "Epoch 462/500 Loss: 0.38518548011779785\n",
      "Epoch 463/500 Loss: 0.27431029081344604\n",
      "Epoch 464/500 Loss: 0.28962987661361694\n",
      "Epoch 465/500 Loss: 0.32723352313041687\n",
      "Epoch 466/500 Loss: 0.17475713789463043\n",
      "Epoch 467/500 Loss: 0.29994598031044006\n",
      "Epoch 468/500 Loss: 0.4559948444366455\n",
      "Epoch 469/500 Loss: 0.6539773344993591\n",
      "Epoch 470/500 Loss: 0.3893490731716156\n",
      "Epoch 471/500 Loss: 0.3309595584869385\n",
      "Epoch 472/500 Loss: 0.5328788161277771\n",
      "Epoch 473/500 Loss: 0.32554057240486145\n",
      "Epoch 474/500 Loss: 0.24104851484298706\n",
      "Epoch 475/500 Loss: 0.2768010199069977\n",
      "Epoch 476/500 Loss: 0.504112958908081\n",
      "Epoch 477/500 Loss: 0.46831414103507996\n",
      "Epoch 478/500 Loss: 0.29048633575439453\n",
      "Epoch 479/500 Loss: 0.4822765588760376\n",
      "Epoch 480/500 Loss: 0.33231276273727417\n",
      "Epoch 481/500 Loss: 0.3289583921432495\n",
      "Epoch 482/500 Loss: 0.5684205889701843\n",
      "Epoch 483/500 Loss: 0.21996250748634338\n",
      "Epoch 484/500 Loss: 0.5643187165260315\n",
      "Epoch 485/500 Loss: 0.23978635668754578\n",
      "Epoch 486/500 Loss: 0.34207284450531006\n",
      "Epoch 487/500 Loss: 0.3499913513660431\n",
      "Epoch 488/500 Loss: 0.18262609839439392\n",
      "Epoch 489/500 Loss: 0.20069074630737305\n",
      "Epoch 490/500 Loss: 0.799172580242157\n",
      "Epoch 491/500 Loss: 0.4626045227050781\n",
      "Epoch 492/500 Loss: 0.3589320480823517\n",
      "Epoch 493/500 Loss: 0.3972662687301636\n",
      "Epoch 494/500 Loss: 0.41527557373046875\n",
      "Epoch 495/500 Loss: 0.1741333305835724\n",
      "Epoch 496/500 Loss: 0.435968816280365\n",
      "Epoch 497/500 Loss: 0.43931499123573303\n",
      "Epoch 498/500 Loss: 0.6429038047790527\n",
      "Epoch 499/500 Loss: 0.6960420608520508\n",
      "Epoch 500/500 Loss: 0.3416023850440979\n"
     ]
    }
   ],
   "source": [
    "model = BinaryClassifier(input_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move inputs and labels to the device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f112f783-3c52-4963-8432-50c79b30a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.40894972966755%\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        predicted = outputs.squeeze().round()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
